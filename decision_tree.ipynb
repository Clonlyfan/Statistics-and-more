{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+L+YkuyaIiXQBrhsBBer0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Clonlyfan/Statistics-and-more/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4qVgeKfdaq_",
        "outputId": "79445049-4bfb-41f5-d28f-766d392d4086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Entropy of the dataset: 1.2638\n",
            "Initial Gini Impurity of the dataset: 0.5408\n",
            "\n",
            "Information Gain for each feature:\n",
            "Information Gain (Outlook): 0.5441\n",
            "Information Gain (Temperature): 0.4441\n",
            "Information Gain (Humidity): 0.4975\n",
            "Information Gain (Wind): 0.3532\n",
            "\n",
            "\n",
            "Gini Gain for each feature:\n",
            "Gini Gain (Outlook): -0.0314\n",
            "Gini Gain (Temperature): 0.1704\n",
            "Gini Gain (Humidity): 0.2234\n",
            "Gini Gain (Wind): 0.1380\n",
            "\n",
            "\n",
            "Detailed Explanation:\n",
            "\n",
            "1. Initial Entropy and Gini Impurity:\n",
            "   - **Entropy** measures the impurity or randomness in the target variable ('Play Tennis') of the entire dataset. A higher entropy value indicates more disorder, meaning the classes (Yes/No) are more mixed.\n",
            "     - Initial Entropy: 1.2638\n",
            "   - **Gini Impurity** is another measure of impurity. It represents the probability of misclassifying a randomly chosen instance if it were randomly labeled according to the class distribution in the dataset. A higher Gini impurity also indicates more disorder.\n",
            "     - Initial Gini Impurity: 0.5408\n",
            "\n",
            "2. Information Gain:\n",
            "   - **Information Gain** quantifies the reduction in entropy achieved by splitting the dataset based on a particular feature. It tells us how much more 'organized' the target variable becomes after partitioning the data according to the values of that feature.\n",
            "   - For each feature ('Outlook', 'Temperature', 'Humidity', 'Wind'):\n",
            "     - We calculate the entropy of the 'Play Tennis' outcome for each unique value within that feature (e.g., for 'Outlook': Sunny, Overcast, Rainy).\n",
            "     - Then, we calculate a weighted average of these entropies, where the weights are the proportion of instances belonging to each value of the feature.\n",
            "     - The Information Gain is the difference between the initial entropy of the dataset and this weighted average entropy. A higher Information Gain suggests that the feature is more effective in classifying the 'Play Tennis' outcome.\n",
            "   - Based on the calculated Information Gains:\n",
            "     - Information Gain (Outlook): 0.5441\n",
            "     - Information Gain (Temperature): 0.4441\n",
            "     - Information Gain (Humidity): 0.4975\n",
            "     - Information Gain (Wind): 0.3532\n",
            "\n",
            "\n",
            "3. Gini Gain:\n",
            "   - **Gini Gain** is analogous to Information Gain but uses Gini impurity instead of entropy. It measures the reduction in Gini impurity achieved by splitting the dataset on a particular feature.\n",
            "   - Similar to Information Gain, a higher Gini Gain for a feature indicates that splitting on that feature leads to a greater reduction in impurity in the resulting subsets, making it a potentially good feature for splitting in a decision tree.\n",
            "   - For each feature:\n",
            "     - Gini Gain (Outlook): -0.0314\n",
            "     - Gini Gain (Temperature): 0.1704\n",
            "     - Gini Gain (Humidity): 0.2234\n",
            "     - Gini Gain (Wind): 0.1380\n",
            "\n",
            "\n",
            "In the context of building a decision tree:\n",
            "- The feature with the **highest Information Gain** (if using entropy as the splitting criterion) or the **highest Gini Gain** (if using Gini impurity) would typically be chosen as the root node of the tree.\n",
            "- This is because these features provide the most information about the target variable and lead to the most homogeneous (pure) child nodes after the split.\n",
            "- The process would then be recursively applied to the child nodes until a stopping criterion is met (e.g., all instances in a node belong to the same class, or a maximum tree depth is reached).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    \"\"\"Calculates the entropy of a dataset.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return 0\n",
        "    probabilities = data['Play Tennis'].value_counts(normalize=True)\n",
        "    entropy = -sum(p * math.log2(p) for p in probabilities if p > 0)\n",
        "    return entropy\n",
        "\n",
        "def calculate_gini_impurity(data):\n",
        "    \"\"\"Calculates the Gini impurity of a dataset.\"\"\"\n",
        "    if len(data) == 0:\n",
        "        return 0\n",
        "    probabilities = data['Play Tennis'].value_counts(normalize=True)\n",
        "    gini = 1 - sum(p**2 for p in probabilities)\n",
        "    return gini\n",
        "\n",
        "def calculate_information_gain(data, attribute):\n",
        "    \"\"\"Calculates the information gain of splitting data on a given attribute.\"\"\"\n",
        "    total_entropy = calculate_entropy(data)\n",
        "    attribute_values = data[attribute].unique()\n",
        "    weighted_entropy = 0\n",
        "\n",
        "    for value in attribute_values:\n",
        "        subset = data[data[attribute] == value]\n",
        "        proportion = len(subset) / len(data)\n",
        "        weighted_entropy += proportion * calculate_entropy(subset)\n",
        "\n",
        "    information_gain = total_entropy - weighted_entropy\n",
        "    return information_gain\n",
        "\n",
        "def calculate_gini_gain(data, attribute):\n",
        "    \"\"\"Calculates the Gini gain of splitting data on a given attribute.\"\"\"\n",
        "    total_gini = calculate_gini_impurity(data)\n",
        "    attribute_values = data[attribute].unique()\n",
        "    weighted_gini = 0\n",
        "\n",
        "    for value in attribute_values:\n",
        "        subset = data[data[attribute] == value]\n",
        "        proportion = len(subset) / len(data)\n",
        "        weighted_gini += proportion * calculate_gini_impurity(subset)\n",
        "\n",
        "    gini_gain = total_gini - weighted_gini\n",
        "    return gini_gain\n",
        "\n",
        "# Load the data from the Excel file\n",
        "file_path = \"/content/gamedayornot.xlsx\"\n",
        "try:\n",
        "    df = pd.read_excel(file_path)\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    exit()\n",
        "\n",
        "# Calculate initial entropy and Gini impurity of the entire dataset\n",
        "initial_entropy = calculate_entropy(df)\n",
        "initial_gini = calculate_gini_impurity(df)\n",
        "\n",
        "print(f\"Initial Entropy of the dataset: {initial_entropy:.4f}\")\n",
        "print(f\"Initial Gini Impurity of the dataset: {initial_gini:.4f}\\n\")\n",
        "\n",
        "features = ['Outlook', 'Temperature', 'Humidity', 'Wind']\n",
        "\n",
        "# Calculate Information Gain for each feature\n",
        "print(\"Information Gain for each feature:\")\n",
        "for feature in features:\n",
        "    info_gain = calculate_information_gain(df, feature)\n",
        "    print(f\"Information Gain ({feature}): {info_gain:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate Gini Gain for each feature\n",
        "print(\"Gini Gain for each feature:\")\n",
        "for feature in features:\n",
        "    gini_gain = calculate_gini_gain(df, feature)\n",
        "    print(f\"Gini Gain ({feature}): {gini_gain:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Detailed Explanation:\n",
        "\n",
        "print(\"Detailed Explanation:\\n\")\n",
        "\n",
        "print(\"1. Initial Entropy and Gini Impurity:\")\n",
        "print(\"   - **Entropy** measures the impurity or randomness in the target variable ('Play Tennis') of the entire dataset. A higher entropy value indicates more disorder, meaning the classes (Yes/No) are more mixed.\")\n",
        "print(f\"     - Initial Entropy: {initial_entropy:.4f}\")\n",
        "print(\"   - **Gini Impurity** is another measure of impurity. It represents the probability of misclassifying a randomly chosen instance if it were randomly labeled according to the class distribution in the dataset. A higher Gini impurity also indicates more disorder.\")\n",
        "print(f\"     - Initial Gini Impurity: {initial_gini:.4f}\\n\")\n",
        "\n",
        "print(\"2. Information Gain:\")\n",
        "print(\"   - **Information Gain** quantifies the reduction in entropy achieved by splitting the dataset based on a particular feature. It tells us how much more 'organized' the target variable becomes after partitioning the data according to the values of that feature.\")\n",
        "print(\"   - For each feature ('Outlook', 'Temperature', 'Humidity', 'Wind'):\")\n",
        "print(\"     - We calculate the entropy of the 'Play Tennis' outcome for each unique value within that feature (e.g., for 'Outlook': Sunny, Overcast, Rainy).\")\n",
        "print(\"     - Then, we calculate a weighted average of these entropies, where the weights are the proportion of instances belonging to each value of the feature.\")\n",
        "print(\"     - The Information Gain is the difference between the initial entropy of the dataset and this weighted average entropy. A higher Information Gain suggests that the feature is more effective in classifying the 'Play Tennis' outcome.\")\n",
        "print(\"   - Based on the calculated Information Gains:\")\n",
        "for feature in features:\n",
        "    info_gain = calculate_information_gain(df, feature)\n",
        "    print(f\"     - Information Gain ({feature}): {info_gain:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"3. Gini Gain:\")\n",
        "print(\"   - **Gini Gain** is analogous to Information Gain but uses Gini impurity instead of entropy. It measures the reduction in Gini impurity achieved by splitting the dataset on a particular feature.\")\n",
        "print(\"   - Similar to Information Gain, a higher Gini Gain for a feature indicates that splitting on that feature leads to a greater reduction in impurity in the resulting subsets, making it a potentially good feature for splitting in a decision tree.\")\n",
        "print(\"   - For each feature:\")\n",
        "for feature in features:\n",
        "    gini_gain = calculate_gini_gain(df, feature)\n",
        "    print(f\"     - Gini Gain ({feature}): {gini_gain:.4f}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"In the context of building a decision tree:\")\n",
        "print(\"- The feature with the **highest Information Gain** (if using entropy as the splitting criterion) or the **highest Gini Gain** (if using Gini impurity) would typically be chosen as the root node of the tree.\")\n",
        "print(\"- This is because these features provide the most information about the target variable and lead to the most homogeneous (pure) child nodes after the split.\")\n",
        "print(\"- The process would then be recursively applied to the child nodes until a stopping criterion is met (e.g., all instances in a node belong to the same class, or a maximum tree depth is reached).\")"
      ]
    }
  ]
}